{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"1mRFyKihoCo48bhz7I7YvVFZbaOZDUEPr","authorship_tag":"ABX9TyPl4zNuL6guPjMfX+QToANI"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"id":"0oW07viD7M4n","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1754463783261,"user_tz":-330,"elapsed":20321,"user":{"displayName":"Apurva Malasi","userId":"05568717401926727542"}},"outputId":"39844d6f-50cd-4713-c139-928fb1930d6b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["# Cell 1: Mount Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["# Cell 2: Necessary Imports\n","import os\n","import random\n","import numpy as np\n","from PIL import Image\n","import json\n","from collections import defaultdict\n","from sklearn.model_selection import train_test_split\n","\n","import tensorflow as tf\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.applications import EfficientNetV2M, EfficientNetV2L\n","from tensorflow.keras.layers import Input, GlobalAveragePooling2D, Dense, Concatenate, Reshape, Multiply, Add\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.optimizers import Adam, AdamW\n","from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n","from tensorflow.keras.layers import Conv2D\n","\n","# Set random seed for reproducibility\n","tf.random.set_seed(42)\n","np.random.seed(42)\n","random.seed(42)"],"metadata":{"id":"sHc_10XcKUhX","executionInfo":{"status":"ok","timestamp":1754465219397,"user_tz":-330,"elapsed":3730,"user":{"displayName":"Apurva Malasi","userId":"05568717401926727542"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["# Cell 3: Paste the Custom Data Generator Code\n","# --- 1. SET UP YOUR ENVIRONMENT AND PATHS ---\n","# This code assumes you have mounted your Google Drive in Colab\n","# and set up the folder structure as recommended.\n","\n","# Path to the root of your project in Google Drive\n","# IMPORTANT: Make sure your Google Drive is mounted first!\n","from google.colab import drive\n","drive.mount('/content/drive')\n","project_root = '/content/drive/My Drive/ml_drone_project'\n","\n","# Paths to your datasets\n","ROBOFLOW_THERMAL_PATH = os.path.join(project_root, 'datasets', 'roboflow_thermal_dogs_people')\n","HIT_UAV_PATH = os.path.join(project_root, 'datasets', 'hit_uav')\n","COCO_ANIMALS_PEOPLE_PATH = os.path.join(project_root, 'datasets', 'coco_animals_people')\n","\n","# Define target image size for the model\n","TARGET_IMAGE_SIZE = (224, 224) # EfficientNetV2-M/L default input size\n","BATCH_SIZE = 32\n","RANDOM_SEED = 42\n","\n","# Define class mappings\n","# Our final model will classify into 2 classes: Human (0) and Animal (1)\n","CLASS_MAP = {'human': 0, 'animal': 1}\n","REVERSE_CLASS_MAP = {0: 'human', 1: 'animal'}\n","\n","# COCO category IDs for 'person' and various animals\n","# You can customize which animals you want to include from COCO\n","COCO_PERSON_ID = 1\n","COCO_ANIMAL_IDS = [\n","    15,  # bird\n","    16,  # cat\n","    17,  # dog\n","    18,  # horse\n","    19,  # sheep\n","    20,  # cow\n","    21,  # elephant\n","    22,  # bear\n","    23,  # zebra\n","    24   # giraffe\n","]\n","# Combine all relevant COCO IDs for filtering\n","COCO_TARGET_IDS = [COCO_PERSON_ID] + COCO_ANIMAL_IDS\n","\n","# --- 2. HELPER FUNCTIONS FOR IMAGE LOADING AND PREPROCESSING ---\n","\n","def load_ir_image(image_path, target_size=TARGET_IMAGE_SIZE):\n","    \"\"\"\n","    Loads a thermal image (typically 8-bit or 16-bit grayscale), resizes it,\n","    normalizes to [0, 1], and ensures 3 channels.\n","    \"\"\"\n","    try:\n","        image = Image.open(image_path)\n","        image = image.resize(target_size, Image.Resampling.LANCZOS)\n","        image_array = np.array(image, dtype=np.float32)\n","\n","        # Normalize\n","        if np.max(image_array) > 0:\n","            image_array = image_array / np.max(image_array)\n","        else:\n","            image_array = np.zeros_like(image_array)\n","\n","        # Ensure 3 channels\n","        if image_array.ndim == 2:  # Grayscale\n","            image_array = np.stack([image_array] * 3, axis=-1)\n","        elif image_array.ndim == 3 and image_array.shape[2] == 1:  # (H, W, 1)\n","            image_array = np.repeat(image_array, 3, axis=-1)\n","        elif image_array.ndim == 3 and image_array.shape[2] == 3:\n","            pass  # Already valid\n","        else:\n","            print(f\"Unexpected IR image shape after loading: {image_array.shape}\")\n","            return None\n","\n","        return image_array\n","\n","    except Exception as e:\n","        print(f\"Error loading IR image {image_path}: {e}\")\n","        return None\n","\n","def load_rgb_image(image_path, target_size=TARGET_IMAGE_SIZE):\n","    \"\"\"\n","    Loads a standard RGB image, resizes it, and normalizes it to [0, 1].\n","    \"\"\"\n","    try:\n","        image = Image.open(image_path).convert('RGB') # Ensure 3 channels\n","        image = image.resize(target_size, Image.Resampling.LANCZOS)\n","        image_array = np.array(image, dtype=np.float32)\n","\n","        # Normalize RGB images to 0-1 range\n","        image_array = image_array / 255.0\n","\n","        return image_array\n","\n","    except Exception as e:\n","        print(f\"Error loading RGB image {image_path}: {e}\")\n","        return None\n","\n","\n","# --- 3. ANNOTATION PARSING FUNCTIONS ---\n","\n","def parse_roboflow_annotations(annotations_file_path):\n","    \"\"\"\n","    Parses Roboflow-style JSON annotation file and returns image paths and labels.\n","    Assumes images are in the same directory as the JSON.\n","    \"\"\"\n","    image_info_list = []\n","\n","    try:\n","        # Check if annotation file exists\n","        if not os.path.exists(annotations_file_path):\n","            print(f\"Warning: Annotation file {annotations_file_path} does not exist\")\n","            return image_info_list\n","\n","        with open(annotations_file_path, 'r') as f:\n","            data = json.load(f)\n","\n","        # Debug: Print the structure of the JSON\n","        print(f\"Parsing {annotations_file_path}\")\n","        print(f\"JSON keys: {list(data.keys())}\")\n","\n","        if 'categories' in data:\n","            print(f\"Categories: {[(cat['id'], cat['name']) for cat in data['categories']]}\")\n","\n","        if 'images' in data:\n","            print(f\"Total images in annotation: {len(data['images'])}\")\n","\n","        if 'annotations' in data:\n","            print(f\"Total annotations: {len(data['annotations'])}\")\n","\n","        # We need to get the parent directory for image loading\n","        base_dir = os.path.dirname(annotations_file_path)\n","        print(f\"Looking for images in: {base_dir}\")\n","\n","        # Check what files actually exist in the directory\n","        if os.path.exists(base_dir):\n","            actual_files = [f for f in os.listdir(base_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n","            print(f\"Actual image files found: {len(actual_files)}\")\n","            if len(actual_files) > 0:\n","                print(f\"Sample files: {actual_files[:5]}\")\n","\n","        # Create mapping from image_id to file_name\n","        if 'images' not in data:\n","            print(\"Error: No 'images' key in JSON data\")\n","            return image_info_list\n","\n","        image_id_to_filename = {img['id']: img['file_name'] for img in data['images']}\n","\n","        # Track processed images to avoid duplicates\n","        processed_images = set()\n","\n","        for ann in data['annotations']:\n","            image_id = ann['image_id']\n","            category_id = ann['category_id']\n","\n","            # Skip if we've already processed this image\n","            if image_id in processed_images:\n","                continue\n","\n","            # Map category_id to category name\n","            category_name = None\n","            for cat in data['categories']:\n","                if cat['id'] == category_id:\n","                    category_name = cat['name'].lower()\n","                    break\n","\n","            if not category_name:\n","                continue\n","\n","            # Map to our classes\n","            label = None\n","            if category_name in ['person', 'people', 'human']:\n","                label = CLASS_MAP['human']\n","            elif category_name in ['dog', 'animal']:\n","                label = CLASS_MAP['animal']\n","            else:\n","                print(f\"Skipping unknown category: {category_name}\")\n","                continue\n","\n","            image_filename = image_id_to_filename.get(image_id)\n","            if not image_filename:\n","                continue\n","\n","            image_path = os.path.join(base_dir, image_filename)\n","\n","            # CHECK IF FILE EXISTS BEFORE ADDING\n","            if not os.path.exists(image_path):\n","                print(f\"Warning: Image file {image_filename} not found at {image_path}\")\n","                continue\n","\n","            image_info_list.append({'path': image_path, 'label': label})\n","            processed_images.add(image_id)\n","\n","        print(f\"Successfully parsed {len(image_info_list)} images from {annotations_file_path}\")\n","\n","    except Exception as e:\n","        print(f\"Error parsing Roboflow annotations {annotations_file_path}: {e}\")\n","        import traceback\n","        traceback.print_exc()\n","\n","    return image_info_list\n","\n","def parse_hit_uav_labels(labels_dir_path):\n","    \"\"\"\n","    Parses HIT-UAV YOLO-style TXT labels and returns image paths and labels.\n","    Assumes image files are in a sibling 'images' directory.\n","    \"\"\"\n","    image_info_list = []\n","\n","    # Assuming YOLO format: one .txt file per image, containing class_id x_center y_center width height\n","    # And images are in a parallel 'images' directory.\n","\n","    image_base_dir = os.path.join(os.path.dirname(os.path.dirname(labels_dir_path)), 'images', os.path.basename(labels_dir_path))\n","\n","    for label_file in os.listdir(labels_dir_path):\n","        if label_file.endswith('.txt'):\n","            label_path = os.path.join(labels_dir_path, label_file)\n","            image_filename = label_file.replace('.txt', '.jpg') # Assuming .jpg images\n","            image_path = os.path.join(image_base_dir, image_filename)\n","\n","            if not os.path.exists(image_path):\n","                print(f\"Warning: Image {image_path} not found for label {label_file}. Skipping.\")\n","                continue\n","\n","            with open(label_path, 'r') as f:\n","                lines = f.readlines()\n","\n","            # HIT-UAV has 'Person', 'Car', 'Bicycle', 'OtherVehicle', 'DontCare'\n","            # You need to know the mapping of their class IDs to names.\n","            # Assuming 'Person' is class ID 0 in their YOLO format.\n","            found_person = False\n","            for line in lines:\n","                parts = line.strip().split()\n","                if len(parts) > 0:\n","                    class_id = int(parts[0])\n","                    # Assuming HIT-UAV 'Person' class ID is 0\n","                    if class_id == 0: # This is the 'Person' class in HIT-UAV\n","                        found_person = True\n","                        break\n","\n","            if found_person:\n","                image_info_list.append({'path': image_path, 'label': CLASS_MAP['human']})\n","            # HIT-UAV does not have animal classes. We only extract 'human' here.\n","\n","    return image_info_list\n","\n","def parse_coco_annotations(annotations_file_path, images_base_dir):\n","    \"\"\"\n","    Parses COCO-style JSON annotation file, filters for 'person' and animal classes,\n","    and returns image paths and labels.\n","    \"\"\"\n","    image_info_list = []\n","\n","    try:\n","        with open(annotations_file_path, 'r') as f:\n","            data = json.load(f)\n","\n","        # Create a mapping from category ID to category name\n","        category_id_to_name = {cat['id']: cat['name'] for cat in data['categories']}\n","\n","        # Create a mapping from image ID to image file name\n","        image_id_to_filename = {img['id']: img['file_name'] for img in data['images']}\n","\n","        # Store images by their dominant class (human or animal)\n","        # An image can contain multiple objects. We prioritize 'human' if present.\n","        coco_human_images = []\n","        coco_animal_images = []\n","\n","        # Track which images have already been processed to avoid duplicates\n","        processed_image_ids = set()\n","\n","        for ann in data['annotations']:\n","            image_id = ann['image_id']\n","            category_id = ann['category_id']\n","\n","            if image_id in processed_image_ids:\n","                continue # Already processed this image for its dominant class\n","\n","            image_filename = image_id_to_filename.get(image_id)\n","            if not image_filename:\n","                continue # Skip if image file name not found\n","\n","            image_path = os.path.join(images_base_dir, image_filename)\n","            # CHECK IF FILE EXISTS BEFORE ADDING\n","            if not os.path.exists(image_path):\n","                continue  # Skip if image file doesn't exist\n","\n","            if category_id == COCO_PERSON_ID:\n","                coco_human_images.append({'path': image_path, 'label': CLASS_MAP['human']})\n","                processed_image_ids.add(image_id) # Mark as processed for dominant class\n","            elif category_id in COCO_ANIMAL_IDS:\n","                coco_animal_images.append({'path': image_path, 'label': CLASS_MAP['animal']})\n","                processed_image_ids.add(image_id) # Mark as processed for dominant class\n","\n","    except Exception as e:\n","        print(f\"Error parsing COCO annotations {annotations_file_path}: {e}\")\n","\n","    return coco_human_images, coco_animal_images\n","\n","\n","\n","# --- 4. THE CUSTOM DATA GENERATOR CLASS ---\n","\n","class MultiModalDataGenerator(tf.keras.utils.Sequence):\n","    \"\"\"\n","    A custom data generator for our multi-modal model.\n","    It handles loading both IR and RGB images for a given batch,\n","    and applies consistent data augmentation.\n","    \"\"\"\n","    def __init__(self,\n","                 ir_data_info, # List of {'path': ..., 'label': ...} for IR images\n","                 rgb_human_paths, # List of RGB image paths for human class\n","                 rgb_animal_paths, # List of RGB image paths for animal class\n","                 batch_size=BATCH_SIZE,\n","                 target_size=TARGET_IMAGE_SIZE,\n","                 shuffle=True,\n","                 seed=RANDOM_SEED,\n","                 horizontal_flip_prob=0.5\n","                 ):\n","\n","        self.ir_data_info = ir_data_info\n","        self.rgb_human_paths = rgb_human_paths\n","        self.rgb_animal_paths = rgb_animal_paths\n","        self.batch_size = batch_size\n","        self.target_size = target_size\n","        self.shuffle = shuffle\n","        self.seed = seed\n","\n","        self.horizontal_flip_prob = horizontal_flip_prob\n","\n","        self.on_epoch_end() # Initialize indices\n","\n","    def __len__(self):\n","        \"\"\"Returns the number of batches per epoch.\"\"\"\n","        return int(np.floor(len(self.ir_data_info) / self.batch_size))\n","\n","    def __getitem__(self, index):\n","        \"\"\"Generates one batch of data.\"\"\"\n","        batch_indices = self.indices[index * self.batch_size:(index + 1) * self.batch_size]\n","\n","        ir_batch_data_info = [self.ir_data_info[k] for k in batch_indices]\n","\n","        X, y = self.__data_generation(ir_batch_data_info)\n","        return X, y\n","\n","    def on_epoch_end(self):\n","        \"\"\"Updates indices for shuffling after each epoch.\"\"\"\n","        self.indices = np.arange(len(self.ir_data_info))\n","        if self.shuffle:\n","            np.random.seed(self.seed)\n","            np.random.shuffle(self.indices)\n","\n","    def __data_generation(self, ir_batch_data_info):\n","        \"\"\"\n","        Loads, processes, and augments a batch of images.\n","        Dynamically pairs IR images with relevant RGB images.\n","        \"\"\"\n","        batch_rgb = []\n","        batch_ir = []\n","        batch_labels = []\n","\n","        for item in ir_batch_data_info:\n","            # --- FIX 1: Add a check for malformed data ---\n","            # This prevents the initial TypeError you encountered.\n","            if not isinstance(item, dict) or 'path' not in item or 'label' not in item:\n","                print(f\"Warning: Skipping malformed item in data generator: {item}\")\n","                continue\n","\n","            ir_path = item['path']\n","            ir_label = item['label']\n","\n","            # Load IR image\n","            ir_image = load_ir_image(ir_path, self.target_size)\n","            if ir_image is None:\n","                continue # Skip if IR image loading failed\n","\n","            # Select a corresponding RGB image based on the IR image's label\n","            rgb_path = None\n","            if ir_label == CLASS_MAP['human'] and self.rgb_human_paths:\n","                # --- FIX 2: Correctly select the RGB path string ---\n","                # `self.rgb_human_paths` is a list of strings, so `random.choice` returns the string itself.\n","                # The old code `random.choice(...)[path]` would cause a TypeError.\n","                rgb_path = random.choice(self.rgb_human_paths)\n","            elif ir_label == CLASS_MAP['animal'] and self.rgb_animal_paths:\n","                # --- FIX 2 (continued): Correctly select the RGB path string ---\n","                rgb_path = random.choice(self.rgb_animal_paths)\n","\n","            # Load RGB image (or use a black placeholder if no suitable RGB image found)\n","            rgb_image = load_rgb_image(rgb_path, self.target_size) if rgb_path else \\\n","                        np.zeros((*self.target_size, 3), dtype=np.float32)\n","\n","            if rgb_image is None:\n","                continue\n","\n","            # --- Shape validation ---\n","            if ir_image.shape != (224, 224, 3):\n","              print(f\"Invalid IR image shape {ir_image.shape} at: {ir_path}\")\n","              continue\n","            if rgb_image.shape != (224, 224, 3):\n","              print(f\"Invalid RGB image shape {rgb_image.shape} at: {rgb_path}\")\n","              continue\n","\n","            # --- Apply Consistent Data Augmentation ---\n","            random_state = np.random.RandomState(np.random.randint(0, 2**32 - 1))\n","\n","            if random_state.rand() < self.horizontal_flip_prob:\n","                ir_image = np.fliplr(ir_image)\n","                rgb_image = np.fliplr(rgb_image)\n","\n","            batch_ir.append(ir_image)\n","            batch_rgb.append(rgb_image)\n","            batch_labels.append(ir_label)\n","\n","        # Convert to numpy arrays\n","        X_ir = np.array(batch_ir)\n","        X_rgb = np.array(batch_rgb)\n","        y = np.array(batch_labels)\n","\n","        # Return a dictionary of inputs for our multi-input Keras model\n","        return {'ir_input': X_ir, 'rgb_input': X_rgb}, y\n","\n","\n","\n","# --- 5. DATASET PREPARATION AND GENERATOR INSTANTIATION ---\n","\n","def get_all_dataset_info(project_root_path):\n","    \"\"\"\n","    Orchestrates parsing all datasets and preparing data info lists.\n","    \"\"\"\n","    print(\"--- Parsing Roboflow Thermal Dogs and People Dataset ---\")\n","    roboflow_train_ann_file = os.path.join(ROBOFLOW_THERMAL_PATH, 'train', '_annotations.coco.json')\n","    roboflow_val_ann_file = os.path.join(ROBOFLOW_THERMAL_PATH, 'valid', '_annotations.coco.json')\n","    roboflow_test_ann_file = os.path.join(ROBOFLOW_THERMAL_PATH, 'test', '_annotations.coco.json')\n","\n","    roboflow_train_info = parse_roboflow_annotations(roboflow_train_ann_file)\n","    roboflow_val_info = parse_roboflow_annotations(roboflow_val_ann_file)\n","    roboflow_test_info = parse_roboflow_annotations(roboflow_test_ann_file)\n","    print(f\"Roboflow Thermal Train: {len(roboflow_train_info)} images\")\n","    print(f\"Roboflow Thermal Val: {len(roboflow_val_info)} images\")\n","    print(f\"Roboflow Thermal Test: {len(roboflow_test_info)} images\")\n","\n","    print(\"\\n--- Parsing HIT-UAV Dataset ---\")\n","    hit_uav_train_labels_dir = os.path.join(HIT_UAV_PATH, 'labels', 'train')\n","    hit_uav_val_labels_dir = os.path.join(HIT_UAV_PATH, 'labels', 'val')\n","    hit_uav_test_labels_dir = os.path.join(HIT_UAV_PATH, 'labels', 'test')\n","\n","    hit_uav_train_info = parse_hit_uav_labels(hit_uav_train_labels_dir)\n","    hit_uav_val_info = parse_hit_uav_labels(hit_uav_val_labels_dir)\n","    hit_uav_test_info = parse_hit_uav_labels(hit_uav_test_labels_dir)\n","    print(f\"HIT-UAV Train (Humans only): {len(hit_uav_train_info)} images\")\n","    print(f\"HIT-UAV Val (Humans only): {len(hit_uav_val_info)} images\")\n","    print(f\"HIT-UAV Test (Humans only): {len(hit_uav_test_info)} images\")\n","\n","    print(\"\\n--- Parsing COCO Animals/People Dataset ---\")\n","    # Corrected COCO image directory paths to remove '2017'\n","    coco_train_ann_file = os.path.join(COCO_ANIMALS_PEOPLE_PATH, 'annotations', 'instances_train.json')\n","    coco_val_ann_file = os.path.join(COCO_ANIMALS_PEOPLE_PATH, 'annotations', 'instances_val.json')\n","\n","    coco_train_images_dir = os.path.join(COCO_ANIMALS_PEOPLE_PATH, 'train')\n","    coco_val_images_dir = os.path.join(COCO_ANIMALS_PEOPLE_PATH, 'val')\n","\n","    coco_train_humans, coco_train_animals = parse_coco_annotations(coco_train_ann_file, coco_train_images_dir)\n","    coco_val_humans, coco_val_animals = parse_coco_annotations(coco_val_ann_file, coco_val_images_dir)\n","\n","    all_coco_humans = coco_train_humans + coco_val_humans\n","    all_coco_animals = coco_train_animals + coco_val_animals\n","\n","    human_train_val, human_test = train_test_split(all_coco_humans, test_size=0.1, random_state=RANDOM_SEED)\n","    human_train, human_val = train_test_split(human_train_val, test_size=0.111, random_state=RANDOM_SEED)\n","\n","    animal_train_val, animal_test = train_test_split(all_coco_animals, test_size=0.1, random_state=RANDOM_SEED)\n","    animal_train, animal_val = train_test_split(animal_train_val, test_size=0.111, random_state=RANDOM_SEED)\n","\n","    print(f\"COCO Train (new): {len(human_train)} humans, {len(animal_train)} animals\")\n","    print(f\"COCO Val (new): {len(human_val)} humans, {len(animal_val)} animals\")\n","    print(f\"COCO Test (new): {len(human_test)} humans, {len(animal_test)} animals\")\n","\n","    all_ir_train_info = roboflow_train_info + hit_uav_train_info\n","    all_ir_val_info = roboflow_val_info + hit_uav_val_info\n","    all_ir_test_info = roboflow_test_info + hit_uav_test_info\n","\n","    all_rgb_human_paths = [item['path'] for item in human_train + human_val + human_test]\n","    all_rgb_animal_paths = [item['path'] for item in animal_train + animal_val + animal_test]\n","\n","    random.seed(RANDOM_SEED)\n","    random.shuffle(all_rgb_human_paths)\n","    random.shuffle(all_rgb_animal_paths)\n","\n","    print(f\"\\nTotal IR (Train+Val+Test) images: {len(all_ir_train_info) + len(all_ir_val_info) + len(all_ir_test_info)}\")\n","    print(f\"Total RGB Human images available for pairing: {len(all_rgb_human_paths)}\")\n","    print(f\"Total RGB Animal images available for pairing: {len(all_rgb_animal_paths)}\")\n","\n","    train_generator = MultiModalDataGenerator(\n","        ir_data_info=all_ir_train_info,\n","        rgb_human_paths=all_rgb_human_paths,\n","        rgb_animal_paths=all_rgb_animal_paths,\n","        batch_size=BATCH_SIZE,\n","        shuffle=True\n","    )\n","\n","    validation_generator = MultiModalDataGenerator(\n","        ir_data_info=all_ir_val_info,\n","        rgb_human_paths=all_rgb_human_paths,\n","        rgb_animal_paths=all_rgb_animal_paths,\n","        batch_size=BATCH_SIZE,\n","        shuffle=False\n","    )\n","\n","    test_generator = MultiModalDataGenerator(\n","        ir_data_info=all_ir_test_info,\n","        rgb_human_paths=all_rgb_human_paths,\n","        rgb_animal_paths=all_rgb_animal_paths,\n","        batch_size=BATCH_SIZE,\n","        shuffle=False\n","    )\n","\n","    return train_generator, validation_generator, test_generator"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JmSbaH0YKWo1","executionInfo":{"status":"ok","timestamp":1754465256034,"user_tz":-330,"elapsed":34002,"user":{"displayName":"Apurva Malasi","userId":"05568717401926727542"}},"outputId":"27a7e21e-dc58-4bc5-d98d-8f62ba8f9f89"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["# Cell 4: Corrected Model Architecture Definition\n","\n","import tensorflow as tf\n","from tensorflow.keras.layers import Input, Conv2D, Concatenate, GlobalAveragePooling2D, Dense, Dropout\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.applications import EfficientNetV2M\n","\n","# --- Define the two inputs for our model ---\n","ir_input = Input(shape=(224, 224, 3), name='ir_input')\n","rgb_input = Input(shape=(224, 224, 3), name='rgb_input')\n","\n","# --- Load the backbone ONCE with pre-trained weights ---\n","print(\"Loading ONE EfficientNetV2-M backbone with ImageNet weights...\")\n","base_model = EfficientNetV2M(\n","    include_top=False,\n","    weights='imagenet',\n","    input_shape=(224, 224, 3) # Specify input_shape for clarity\n",")\n","base_model.trainable = False # Freeze the backbone layers\n","\n","# --- Connect our custom inputs to the SAME backbone ---\n","\n","# Call the *same* base_model instance on both inputs\n","print(\"Reusing the backbone for both IR and RGB streams...\")\n","ir_features = base_model(ir_input)\n","rgb_features = base_model(rgb_input)\n","\n","# --- Feature Fusion ---\n","fused_features = Concatenate(axis=-1)([ir_features, rgb_features])\n","\n","# --- Shared Classification Head ---\n","x = GlobalAveragePooling2D()(fused_features)\n","x = Dense(256, activation='relu')(x)\n","x = Dropout(0.3)(x)\n","output = Dense(1, activation='sigmoid', name='output_classification')(x)\n","\n","# --- Create the final model ---\n","model = Model(inputs=[ir_input, rgb_input], outputs=output)\n","\n","print(\"\\nModel built successfully. ✅\")\n","model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":585},"id":"bpRQqAckKqvs","executionInfo":{"status":"ok","timestamp":1754465272021,"user_tz":-330,"elapsed":7552,"user":{"displayName":"Apurva Malasi","userId":"05568717401926727542"}},"outputId":"be3870e7-92ea-4432-9f5d-637c86d2d00c"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading ONE EfficientNetV2-M backbone with ImageNet weights...\n","Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/efficientnet_v2/efficientnetv2-m_notop.h5\n","\u001b[1m214201816/214201816\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n","Reusing the backbone for both IR and RGB streams...\n","\n","Model built successfully. ✅\n"]},{"output_type":"display_data","data":{"text/plain":["\u001b[1mModel: \"functional\"\u001b[0m\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n","│ ir_input            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ -                 │\n","│ (\u001b[38;5;33mInputLayer\u001b[0m)        │ \u001b[38;5;34m3\u001b[0m)                │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ rgb_input           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ -                 │\n","│ (\u001b[38;5;33mInputLayer\u001b[0m)        │ \u001b[38;5;34m3\u001b[0m)                │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ efficientnetv2-m    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m,      │ \u001b[38;5;34m53,150,388\u001b[0m │ ir_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],   │\n","│ (\u001b[38;5;33mFunctional\u001b[0m)        │ \u001b[38;5;34m1280\u001b[0m)             │            │ rgb_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ concatenate         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m,      │          \u001b[38;5;34m0\u001b[0m │ efficientnetv2-m… │\n","│ (\u001b[38;5;33mConcatenate\u001b[0m)       │ \u001b[38;5;34m2560\u001b[0m)             │            │ efficientnetv2-m… │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ global_average_poo… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2560\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n","│ (\u001b[38;5;33mGlobalAveragePool…\u001b[0m │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ dense (\u001b[38;5;33mDense\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │    \u001b[38;5;34m655,616\u001b[0m │ global_average_p… │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ dropout (\u001b[38;5;33mDropout\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ output_classificat… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │        \u001b[38;5;34m257\u001b[0m │ dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n","│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n","└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n","│ ir_input            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ rgb_input           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ efficientnetv2-m    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>,      │ <span style=\"color: #00af00; text-decoration-color: #00af00\">53,150,388</span> │ ir_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],   │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)             │            │ rgb_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ concatenate         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>,      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ efficientnetv2-m… │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │ <span style=\"color: #00af00; text-decoration-color: #00af00\">2560</span>)             │            │ efficientnetv2-m… │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ global_average_poo… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2560</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePool…</span> │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │    <span style=\"color: #00af00; text-decoration-color: #00af00\">655,616</span> │ global_average_p… │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ output_classificat… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">257</span> │ dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n","└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Total params: \u001b[0m\u001b[38;5;34m53,806,261\u001b[0m (205.25 MB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">53,806,261</span> (205.25 MB)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m655,873\u001b[0m (2.50 MB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">655,873</span> (2.50 MB)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m53,150,388\u001b[0m (202.75 MB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">53,150,388</span> (202.75 MB)\n","</pre>\n"]},"metadata":{}}]},{"cell_type":"code","source":["# Cell 5: Stage 1 - Compile and Train the Head\n","print(\"Stage 1: Training the classification head...\")\n","\n","train_generator, validation_generator, test_generator = get_all_dataset_info(project_root)\n","\n","# Compile the model with a standard learning rate\n","model.compile(\n","    optimizer=Adam(learning_rate=1e-3),\n","    loss='binary_crossentropy',\n","    metrics=[\n","        'accuracy',\n","        tf.keras.metrics.Precision(name='precision'),\n","        tf.keras.metrics.Recall(name='recall'),\n","        tf.keras.metrics.AUC(name='auc')  # Optional\n","    ]\n",")\n","\n","# Set up callbacks\n","checkpoint_head = ModelCheckpoint(\n","    os.path.join(project_root, 'models', 'checkpoints', 'best_model_head.h5'),\n","    monitor='val_accuracy',\n","    save_best_only=True,\n","    mode='max',\n","    verbose=1\n",")\n","early_stopping = EarlyStopping(\n","    monitor='val_loss',\n","    patience=5,\n","    restore_best_weights=True,\n","    verbose=1\n",")\n","\n","class_weight = {0: 1., 1: 3.}  # Adjust this weight as needed\n","\n","from sklearn.utils.class_weight import compute_class_weight\n","import numpy as np\n","\n","# Your class labels from IR training set\n","labels = [item['label'] for item in train_generator.ir_data_info]\n","\n","# Automatically compute balanced weights\n","class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(labels), y=labels)\n","\n","# Convert to dict for Keras\n","class_weight_dict = {i: class_weights[i] for i in range(len(class_weights))}\n","\n","print(\"Using class weights:\", class_weight_dict)\n","\n","\n","# Train for a few epochs with frozen backbones\n","history_head = model.fit(\n","    train_generator,\n","    validation_data=validation_generator,\n","    epochs=10,\n","    callbacks=[checkpoint_head, early_stopping],\n","    class_weight=class_weight_dict\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-tme3NudKuLj","executionInfo":{"status":"ok","timestamp":1754427073926,"user_tz":-330,"elapsed":7334423,"user":{"displayName":"Apurva Malasi","userId":"05568717401926727542"}},"outputId":"70583801-f697-45f4-e145-f95c47635432"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Stage 1: Training the classification head...\n","--- Parsing Roboflow Thermal Dogs and People Dataset ---\n","Parsing /content/drive/My Drive/ml_drone_project/datasets/roboflow_thermal_dogs_people/train/_annotations.coco.json\n","JSON keys: ['info', 'licenses', 'categories', 'images', 'annotations']\n","Categories: [(0, 'dogs-person'), (1, 'dog'), (2, 'person')]\n","Total images in annotation: 142\n","Total annotations: 181\n","Looking for images in: /content/drive/My Drive/ml_drone_project/datasets/roboflow_thermal_dogs_people/train\n","Actual image files found: 142\n","Sample files: ['IMG_0094_jpg.rf.03dbd175cdbd5379608debcc783a5361.jpg', 'IMG_0054_jpg.rf.03e0fd11bad6afeb085f2f156d7fd043.jpg', 'IMG_0056_jpg.rf.0255dca8946ea0a34592394acd67b11b.jpg', 'IMG_0109_jpg.rf.03607def018d49330ebef9a856f65812.jpg', 'IMG_0112_jpg.rf.016d04c2af3bc0221a5153d6af8b9f30.jpg']\n","Successfully parsed 131 images from /content/drive/My Drive/ml_drone_project/datasets/roboflow_thermal_dogs_people/train/_annotations.coco.json\n","Parsing /content/drive/My Drive/ml_drone_project/datasets/roboflow_thermal_dogs_people/valid/_annotations.coco.json\n","JSON keys: ['info', 'licenses', 'categories', 'images', 'annotations']\n","Categories: [(0, 'dogs-person'), (1, 'dog'), (2, 'person')]\n","Total images in annotation: 41\n","Total annotations: 49\n","Looking for images in: /content/drive/My Drive/ml_drone_project/datasets/roboflow_thermal_dogs_people/valid\n","Actual image files found: 41\n","Sample files: ['IMG_0014 3_jpg.rf.131eb12843ba9f462d7d00596b9169ca.jpg', 'IMG_0008 2_jpg.rf.12590c06dd9458d89e73ee35f8037c3d.jpg', 'IMG_0092_jpg.rf.2142d9a1b054620ba888c3073c6a6ef2.jpg', 'IMG_0002_jpg.rf.2698533a21de04406056050cd2796682.jpg', 'IMG_0075_jpg.rf.08c27e14cdba0e87255c59f5eeef5d19.jpg']\n","Successfully parsed 36 images from /content/drive/My Drive/ml_drone_project/datasets/roboflow_thermal_dogs_people/valid/_annotations.coco.json\n","Parsing /content/drive/My Drive/ml_drone_project/datasets/roboflow_thermal_dogs_people/test/_annotations.coco.json\n","JSON keys: ['info', 'licenses', 'categories', 'images', 'annotations']\n","Categories: [(0, 'dogs-person'), (1, 'dog'), (2, 'person')]\n","Total images in annotation: 20\n","Total annotations: 27\n","Looking for images in: /content/drive/My Drive/ml_drone_project/datasets/roboflow_thermal_dogs_people/test\n","Actual image files found: 20\n","Sample files: ['IMG_0033 2_jpg.rf.1ce012dff1ceb8d37c2ee1edd005843b.jpg', 'IMG_0006 5_jpg.rf.cd46e6a862d6ffb7fce6795067ce7cc7.jpg', 'IMG_0031 2_jpg.rf.0872acd4c7cf3d7de868b3ab2d7e464f.jpg', 'IMG_0106_jpg.rf.a36615f0dcd4336c8340cf7808243766.jpg', 'IMG_0027_jpg.rf.888f9a5d05a43267ac39a7c09cd0fc4b.jpg']\n","Successfully parsed 18 images from /content/drive/My Drive/ml_drone_project/datasets/roboflow_thermal_dogs_people/test/_annotations.coco.json\n","Roboflow Thermal Train: 131 images\n","Roboflow Thermal Val: 36 images\n","Roboflow Thermal Test: 18 images\n","\n","--- Parsing HIT-UAV Dataset ---\n","HIT-UAV Train (Humans only): 1165 images\n","HIT-UAV Val (Humans only): 171 images\n","HIT-UAV Test (Humans only): 355 images\n","\n","--- Parsing COCO Animals/People Dataset ---\n","COCO Train (new): 5413 humans, 2032 animals\n","COCO Val (new): 676 humans, 254 animals\n","COCO Test (new): 677 humans, 255 animals\n","\n","Total IR (Train+Val+Test) images: 1876\n","Total RGB Human images available for pairing: 6766\n","Total RGB Animal images available for pairing: 2541\n","Using class weights: {0: np.float64(0.5272579332790887), 1: np.float64(9.671641791044776)}\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n","  self._warn_if_super_not_called()\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21s/step - accuracy: 0.8551 - auc: 0.5613 - loss: 0.8636 - precision: 0.0803 - recall: 0.1980 \n","Epoch 1: val_accuracy improved from -inf to 0.09375, saving model to /content/drive/My Drive/ml_drone_project/models/checkpoints/best_model_head.h5\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1086s\u001b[0m 25s/step - accuracy: 0.8531 - auc: 0.5608 - loss: 0.8606 - precision: 0.0802 - recall: 0.2005 - val_accuracy: 0.0938 - val_auc: 0.5859 - val_loss: 0.7272 - val_precision: 0.0938 - val_recall: 1.0000\n","Epoch 2/10\n","\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21s/step - accuracy: 0.5661 - auc: 0.4977 - loss: 0.7290 - precision: 0.0687 - recall: 0.5225 \n","Epoch 2: val_accuracy improved from 0.09375 to 0.88021, saving model to /content/drive/My Drive/ml_drone_project/models/checkpoints/best_model_head.h5\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m971s\u001b[0m 24s/step - accuracy: 0.5663 - auc: 0.4973 - loss: 0.7285 - precision: 0.0683 - recall: 0.5205 - val_accuracy: 0.8802 - val_auc: 0.5720 - val_loss: 0.6674 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n","Epoch 3/10\n","\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21s/step - accuracy: 0.5839 - auc: 0.4270 - loss: 0.7382 - precision: 0.0420 - recall: 0.3194 \n","Epoch 3: val_accuracy did not improve from 0.88021\n","\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m977s\u001b[0m 24s/step - accuracy: 0.5808 - auc: 0.4282 - loss: 0.7373 - precision: 0.0422 - recall: 0.3242 - val_accuracy: 0.0938 - val_auc: 0.6657 - val_loss: 0.7326 - val_precision: 0.0938 - val_recall: 1.0000\n","Epoch 4/10\n","\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21s/step - accuracy: 0.5755 - auc: 0.4604 - loss: 0.7212 - precision: 0.0438 - recall: 0.3235 \n","Epoch 4: val_accuracy did not improve from 0.88021\n","\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m973s\u001b[0m 24s/step - accuracy: 0.5746 - auc: 0.4621 - loss: 0.7204 - precision: 0.0441 - recall: 0.3278 - val_accuracy: 0.1042 - val_auc: 0.6114 - val_loss: 0.7230 - val_precision: 0.0947 - val_recall: 1.0000\n","Epoch 5/10\n","\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21s/step - accuracy: 0.6312 - auc: 0.5749 - loss: 0.6835 - precision: 0.0692 - recall: 0.5202 \n","Epoch 5: val_accuracy did not improve from 0.88021\n","\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1000s\u001b[0m 25s/step - accuracy: 0.6320 - auc: 0.5738 - loss: 0.6843 - precision: 0.0690 - recall: 0.5172 - val_accuracy: 0.0938 - val_auc: 0.5865 - val_loss: 0.7449 - val_precision: 0.0938 - val_recall: 1.0000\n","Epoch 6/10\n","\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21s/step - accuracy: 0.2815 - auc: 0.4062 - loss: 0.7659 - precision: 0.0395 - recall: 0.5537 \n","Epoch 6: val_accuracy did not improve from 0.88021\n","\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1050s\u001b[0m 25s/step - accuracy: 0.2822 - auc: 0.4081 - loss: 0.7648 - precision: 0.0397 - recall: 0.5560 - val_accuracy: 0.0938 - val_auc: 0.4301 - val_loss: 0.7537 - val_precision: 0.0938 - val_recall: 1.0000\n","Epoch 7/10\n","\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22s/step - accuracy: 0.4182 - auc: 0.4938 - loss: 0.7539 - precision: 0.0582 - recall: 0.6199 \n","Epoch 7: val_accuracy did not improve from 0.88021\n","\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1065s\u001b[0m 25s/step - accuracy: 0.4185 - auc: 0.4948 - loss: 0.7525 - precision: 0.0581 - recall: 0.6199 - val_accuracy: 0.0938 - val_auc: 0.6344 - val_loss: 0.7288 - val_precision: 0.0938 - val_recall: 1.0000\n","Epoch 7: early stopping\n","Restoring model weights from the end of the best epoch: 2.\n"]}]},{"cell_type":"code","source":["print(\"\\nStage 2: Fine-tuning the entire model...\")\n","\n","# Unfreeze the backbones\n","model.trainable = True\n","\n","# ✅ Recompile the model with a very low LR and better metrics\n","from tensorflow.keras.metrics import AUC\n","\n","train_generator, validation_generator, test_generator = get_all_dataset_info(project_root)\n","\n","# 3. Define focal loss\n","def focal_loss(gamma=1.5, alpha=0.4):\n","    def focal_loss_fixed(y_true, y_pred):\n","        eps = tf.keras.backend.epsilon()\n","        y_pred = tf.keras.backend.clip(y_pred, eps, 1. - eps)\n","        pt = tf.where(tf.equal(y_true, 1), y_pred, 1 - y_pred)\n","        return -alpha * tf.keras.backend.pow(1. - pt, gamma) * tf.keras.backend.log(pt)\n","    return focal_loss_fixed\n","\n","# Set up callbacks\n","checkpoint_head = ModelCheckpoint(\n","    os.path.join(project_root, 'models', 'checkpoints', 'best_model_head.h5'),\n","    monitor='val_accuracy',\n","    save_best_only=True,\n","    mode='max',\n","    verbose=1\n",")\n","early_stopping = EarlyStopping(\n","    monitor='val_loss',\n","    patience=5,\n","    restore_best_weights=True,\n","    verbose=1\n",")\n","\n","\n","model.compile(\n","    optimizer=AdamW(learning_rate=1e-5),\n","    loss=focal_loss(gamma=1.5, alpha=0.4),\n","    metrics=[\n","        'accuracy',\n","        tf.keras.metrics.Precision(name='precision'),\n","        tf.keras.metrics.Recall(name='recall'),\n","        AUC(name='auc')  # ✅ Add AUC to track class separation\n","    ]\n",")\n","\n","# ✅ Recompute class weights for fine-tuning\n","from sklearn.utils.class_weight import compute_class_weight\n","import numpy as np\n","\n","fine_tune_labels = [item['label'] for item in train_generator.ir_data_info]\n","class_weights_ft = compute_class_weight(\n","    class_weight='balanced',\n","    classes=np.unique(fine_tune_labels),\n","    y=fine_tune_labels\n",")\n","class_weight_dict_ft = {i: class_weights_ft[i] for i in range(len(class_weights_ft))}\n","print(\"Class weights for fine-tuning:\", class_weight_dict_ft)\n","\n","# ✅ Set up new callbacks for fine-tuning\n","checkpoint_ft = ModelCheckpoint(\n","    os.path.join(project_root, 'models', 'checkpoints', 'best_model_finetuned.keras'),  # use .keras format\n","    monitor='val_accuracy',\n","    save_best_only=True,\n","    mode='max',\n","    verbose=1\n",")\n","\n","reduce_lr = ReduceLROnPlateau(\n","    monitor='val_loss',\n","    factor=0.2,\n","    patience=3,\n","    min_lr=1e-7,\n","    verbose=1\n",")\n","\n","# ✅ Continue training with class weights\n","history_ft = model.fit(\n","    train_generator,\n","    validation_data=validation_generator,\n","    epochs=20,\n","    callbacks=[checkpoint_ft, reduce_lr, early_stopping],\n","    class_weight=class_weight_dict_ft  # ✅ CRITICAL: Apply class weights again\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wXHVqLs9Kx2n","executionInfo":{"status":"ok","timestamp":1754471072272,"user_tz":-330,"elapsed":714780,"user":{"displayName":"Apurva Malasi","userId":"05568717401926727542"}},"outputId":"f3dd4ce0-1fd2-4063-aa6d-31dd936f64a1"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Stage 2: Fine-tuning the entire model...\n","--- Parsing Roboflow Thermal Dogs and People Dataset ---\n","Parsing /content/drive/My Drive/ml_drone_project/datasets/roboflow_thermal_dogs_people/train/_annotations.coco.json\n","JSON keys: ['info', 'licenses', 'categories', 'images', 'annotations']\n","Categories: [(0, 'dogs-person'), (1, 'dog'), (2, 'person')]\n","Total images in annotation: 142\n","Total annotations: 181\n","Looking for images in: /content/drive/My Drive/ml_drone_project/datasets/roboflow_thermal_dogs_people/train\n","Actual image files found: 142\n","Sample files: ['IMG_0094_jpg.rf.03dbd175cdbd5379608debcc783a5361.jpg', 'IMG_0054_jpg.rf.03e0fd11bad6afeb085f2f156d7fd043.jpg', 'IMG_0056_jpg.rf.0255dca8946ea0a34592394acd67b11b.jpg', 'IMG_0109_jpg.rf.03607def018d49330ebef9a856f65812.jpg', 'IMG_0112_jpg.rf.016d04c2af3bc0221a5153d6af8b9f30.jpg']\n","Successfully parsed 131 images from /content/drive/My Drive/ml_drone_project/datasets/roboflow_thermal_dogs_people/train/_annotations.coco.json\n","Parsing /content/drive/My Drive/ml_drone_project/datasets/roboflow_thermal_dogs_people/valid/_annotations.coco.json\n","JSON keys: ['info', 'licenses', 'categories', 'images', 'annotations']\n","Categories: [(0, 'dogs-person'), (1, 'dog'), (2, 'person')]\n","Total images in annotation: 41\n","Total annotations: 49\n","Looking for images in: /content/drive/My Drive/ml_drone_project/datasets/roboflow_thermal_dogs_people/valid\n","Actual image files found: 41\n","Sample files: ['IMG_0014 3_jpg.rf.131eb12843ba9f462d7d00596b9169ca.jpg', 'IMG_0008 2_jpg.rf.12590c06dd9458d89e73ee35f8037c3d.jpg', 'IMG_0092_jpg.rf.2142d9a1b054620ba888c3073c6a6ef2.jpg', 'IMG_0002_jpg.rf.2698533a21de04406056050cd2796682.jpg', 'IMG_0075_jpg.rf.08c27e14cdba0e87255c59f5eeef5d19.jpg']\n","Successfully parsed 36 images from /content/drive/My Drive/ml_drone_project/datasets/roboflow_thermal_dogs_people/valid/_annotations.coco.json\n","Parsing /content/drive/My Drive/ml_drone_project/datasets/roboflow_thermal_dogs_people/test/_annotations.coco.json\n","JSON keys: ['info', 'licenses', 'categories', 'images', 'annotations']\n","Categories: [(0, 'dogs-person'), (1, 'dog'), (2, 'person')]\n","Total images in annotation: 20\n","Total annotations: 27\n","Looking for images in: /content/drive/My Drive/ml_drone_project/datasets/roboflow_thermal_dogs_people/test\n","Actual image files found: 20\n","Sample files: ['IMG_0033 2_jpg.rf.1ce012dff1ceb8d37c2ee1edd005843b.jpg', 'IMG_0006 5_jpg.rf.cd46e6a862d6ffb7fce6795067ce7cc7.jpg', 'IMG_0031 2_jpg.rf.0872acd4c7cf3d7de868b3ab2d7e464f.jpg', 'IMG_0106_jpg.rf.a36615f0dcd4336c8340cf7808243766.jpg', 'IMG_0027_jpg.rf.888f9a5d05a43267ac39a7c09cd0fc4b.jpg']\n","Successfully parsed 18 images from /content/drive/My Drive/ml_drone_project/datasets/roboflow_thermal_dogs_people/test/_annotations.coco.json\n","Roboflow Thermal Train: 131 images\n","Roboflow Thermal Val: 36 images\n","Roboflow Thermal Test: 18 images\n","\n","--- Parsing HIT-UAV Dataset ---\n","HIT-UAV Train (Humans only): 1165 images\n","HIT-UAV Val (Humans only): 171 images\n","HIT-UAV Test (Humans only): 355 images\n","\n","--- Parsing COCO Animals/People Dataset ---\n","COCO Train (new): 5413 humans, 2032 animals\n","COCO Val (new): 676 humans, 254 animals\n","COCO Test (new): 677 humans, 255 animals\n","\n","Total IR (Train+Val+Test) images: 1876\n","Total RGB Human images available for pairing: 6766\n","Total RGB Animal images available for pairing: 2541\n","Class weights for fine-tuning: {0: np.float64(0.5272579332790887), 1: np.float64(9.671641791044776)}\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n","  self._warn_if_super_not_called()\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/20\n","\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 746ms/step - accuracy: 0.5487 - auc: 0.5109 - loss: 0.1054 - precision: 0.0589 - recall: 0.4489\n","Epoch 1: val_accuracy improved from -inf to 0.09375, saving model to /content/drive/My Drive/ml_drone_project/models/checkpoints/best_model_finetuned.keras\n","\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m205s\u001b[0m 2s/step - accuracy: 0.5481 - auc: 0.5118 - loss: 0.1052 - precision: 0.0588 - recall: 0.4512 - val_accuracy: 0.0938 - val_auc: 0.6312 - val_loss: 0.1032 - val_precision: 0.0938 - val_recall: 1.0000 - learning_rate: 1.0000e-05\n","Epoch 2/20\n","\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 869ms/step - accuracy: 0.5118 - auc: 0.5011 - loss: 0.1018 - precision: 0.0560 - recall: 0.4846\n","Epoch 2: val_accuracy did not improve from 0.09375\n","\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 1s/step - accuracy: 0.5120 - auc: 0.5012 - loss: 0.1017 - precision: 0.0560 - recall: 0.4853 - val_accuracy: 0.0938 - val_auc: 0.5056 - val_loss: 0.1027 - val_precision: 0.0938 - val_recall: 1.0000 - learning_rate: 1.0000e-05\n","Epoch 3/20\n","\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 810ms/step - accuracy: 0.5407 - auc: 0.5674 - loss: 0.1003 - precision: 0.0666 - recall: 0.5386\n","Epoch 3: val_accuracy did not improve from 0.09375\n","\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 942ms/step - accuracy: 0.5402 - auc: 0.5678 - loss: 0.1002 - precision: 0.0665 - recall: 0.5399 - val_accuracy: 0.0938 - val_auc: 0.5421 - val_loss: 0.1023 - val_precision: 0.0938 - val_recall: 1.0000 - learning_rate: 1.0000e-05\n","Epoch 4/20\n","\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 795ms/step - accuracy: 0.5391 - auc: 0.5833 - loss: 0.0994 - precision: 0.0715 - recall: 0.6407\n","Epoch 4: val_accuracy did not improve from 0.09375\n","\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 939ms/step - accuracy: 0.5379 - auc: 0.5821 - loss: 0.0994 - precision: 0.0712 - recall: 0.6391 - val_accuracy: 0.0938 - val_auc: 0.6268 - val_loss: 0.1032 - val_precision: 0.0938 - val_recall: 1.0000 - learning_rate: 1.0000e-05\n","Epoch 5/20\n","\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 795ms/step - accuracy: 0.5232 - auc: 0.5507 - loss: 0.0955 - precision: 0.0533 - recall: 0.5114\n","Epoch 5: val_accuracy improved from 0.09375 to 0.16146, saving model to /content/drive/My Drive/ml_drone_project/models/checkpoints/best_model_finetuned.keras\n","\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 1s/step - accuracy: 0.5242 - auc: 0.5503 - loss: 0.0955 - precision: 0.0534 - recall: 0.5104 - val_accuracy: 0.1615 - val_auc: 0.6141 - val_loss: 0.0999 - val_precision: 0.1006 - val_recall: 1.0000 - learning_rate: 1.0000e-05\n","Epoch 6/20\n","\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 940ms/step - accuracy: 0.5399 - auc: 0.5698 - loss: 0.0999 - precision: 0.0662 - recall: 0.5611\n","Epoch 6: val_accuracy did not improve from 0.16146\n","\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 1s/step - accuracy: 0.5396 - auc: 0.5700 - loss: 0.0999 - precision: 0.0661 - recall: 0.5619 - val_accuracy: 0.0990 - val_auc: 0.7219 - val_loss: 0.1031 - val_precision: 0.0942 - val_recall: 1.0000 - learning_rate: 1.0000e-05\n","Epoch 7/20\n","\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 809ms/step - accuracy: 0.5092 - auc: 0.5573 - loss: 0.1021 - precision: 0.0681 - recall: 0.5987\n","Epoch 7: val_accuracy did not improve from 0.16146\n","\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 939ms/step - accuracy: 0.5089 - auc: 0.5569 - loss: 0.1019 - precision: 0.0679 - recall: 0.5981 - val_accuracy: 0.0990 - val_auc: 0.5872 - val_loss: 0.1031 - val_precision: 0.0942 - val_recall: 1.0000 - learning_rate: 1.0000e-05\n","Epoch 8/20\n","\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 801ms/step - accuracy: 0.5134 - auc: 0.5397 - loss: 0.1022 - precision: 0.0629 - recall: 0.5345\n","Epoch 8: val_accuracy did not improve from 0.16146\n","\n","Epoch 8: ReduceLROnPlateau reducing learning rate to 1.9999999494757505e-06.\n","\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 942ms/step - accuracy: 0.5131 - auc: 0.5399 - loss: 0.1021 - precision: 0.0628 - recall: 0.5347 - val_accuracy: 0.0990 - val_auc: 0.6071 - val_loss: 0.1046 - val_precision: 0.0942 - val_recall: 1.0000 - learning_rate: 1.0000e-05\n","Epoch 9/20\n","\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 799ms/step - accuracy: 0.4936 - auc: 0.5539 - loss: 0.0971 - precision: 0.0630 - recall: 0.6250\n","Epoch 9: val_accuracy did not improve from 0.16146\n","\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 953ms/step - accuracy: 0.4934 - auc: 0.5535 - loss: 0.0971 - precision: 0.0629 - recall: 0.6245 - val_accuracy: 0.0938 - val_auc: 0.6746 - val_loss: 0.1044 - val_precision: 0.0938 - val_recall: 1.0000 - learning_rate: 2.0000e-06\n","Epoch 10/20\n","\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 796ms/step - accuracy: 0.4804 - auc: 0.4641 - loss: 0.0937 - precision: 0.0405 - recall: 0.4401\n","Epoch 10: val_accuracy did not improve from 0.16146\n","\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 935ms/step - accuracy: 0.4802 - auc: 0.4653 - loss: 0.0938 - precision: 0.0406 - recall: 0.4412 - val_accuracy: 0.0938 - val_auc: 0.5787 - val_loss: 0.1040 - val_precision: 0.0938 - val_recall: 1.0000 - learning_rate: 2.0000e-06\n","Epoch 10: early stopping\n","Restoring model weights from the end of the best epoch: 5.\n"]}]},{"cell_type":"code","source":["# Cell 7: Final Evaluation\n","print(\"\\nFinal evaluation on the test set...\")\n","\n","train_generator, validation_generator, test_generator = get_all_dataset_info(project_root)\n","\n","# Load the best-performing model from the fine-tuning stage\n","final_model_path = os.path.join(project_root, 'models', 'checkpoints', 'best_model_finetuned.keras')\n","\n","# Redefine the exact focal loss you used during training\n","def focal_loss(gamma=2., alpha=0.75):\n","    def focal_loss_fixed(y_true, y_pred):\n","        eps = tf.keras.backend.epsilon()\n","        y_pred = tf.keras.backend.clip(y_pred, eps, 1. - eps)\n","        pt = tf.where(tf.equal(y_true, 1), y_pred, 1 - y_pred)\n","        return -alpha * tf.keras.backend.pow(1. - pt, gamma) * tf.keras.backend.log(pt)\n","    return focal_loss_fixed\n","\n","# Pass the focal loss into custom_objects when loading the model\n","final_model = tf.keras.models.load_model(\n","    final_model_path,\n","    custom_objects={'focal_loss_fixed': focal_loss(gamma=2., alpha=0.75)}\n",")\n","\n","# Evaluate the model on the unseen test data\n","results = final_model.evaluate(test_generator)\n","\n","print(\"\\n--- Final Test Set Results ---\")\n","print(f\"Loss: {results[0]:.4f}\")\n","print(f\"Accuracy: {results[1]:.4f}\")\n","print(f\"Precision: {results[2]:.4f}\")\n","print(f\"Recall: {results[3]:.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":255},"id":"Qjlvf6tAKzUh","executionInfo":{"status":"error","timestamp":1754471295936,"user_tz":-330,"elapsed":135,"user":{"displayName":"Apurva Malasi","userId":"05568717401926727542"}},"outputId":"e5b4d27f-270d-4e20-e22b-bc9e0199908a"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Final evaluation on the test set...\n"]},{"output_type":"error","ename":"NameError","evalue":"name 'get_all_dataset_info' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-2085872487.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nFinal evaluation on the test set...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrain_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_generator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_all_dataset_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproject_root\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Load the best-performing model from the fine-tuning stage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'get_all_dataset_info' is not defined"]}]},{"cell_type":"code","source":["from collections import Counter\n","print(\"IR Train Labels:\", Counter([item['label'] for item in train_generator.ir_data_info]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PQDOPa0RhNSB","executionInfo":{"status":"ok","timestamp":1754418970807,"user_tz":-330,"elapsed":32,"user":{"displayName":"Apurva Malasi","userId":"05568717401926727542"}},"outputId":"6a827df1-c65c-4c7b-b15f-37746a6ad616"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["IR Train Labels: Counter({0: 1229, 1: 67})\n"]}]},{"cell_type":"code","source":["from sklearn.metrics import precision_score, recall_score, f1_score\n","\n","y_probs = model.predict(test_gen).flatten()\n","y_true = test_gen.labels  # or however you're storing the true labels\n","\n","for threshold in [0.2, 0.3, 0.4, 0.5, 0.6, 0.7]:\n","    y_pred = (y_probs > threshold).astype(int)\n","    print(f\"\\nThreshold = {threshold}\")\n","    print(\"Precision:\", precision_score(y_true, y_pred))\n","    print(\"Recall:\", recall_score(y_true, y_pred))\n","    print(\"F1 Score:\", f1_score(y_true, y_pred))"],"metadata":{"id":"LvMMZ2OgoONl"},"execution_count":null,"outputs":[]}]}